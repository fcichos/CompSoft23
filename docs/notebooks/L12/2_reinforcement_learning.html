<!doctype html>
<html class="no-js" lang="en">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />
<link rel="index" title="Index" href="../../genindex.html" /><link rel="search" title="Search" href="../../search.html" />

    <!-- Generated with Sphinx 6.1.3 and Furo 2023.03.27 -->
        <title>Machine Learning and Neural Networks - Introduction to Computer-based Physical Modeling 23 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/furo.css?digest=fad236701ea90a88636c2a8c73b44ae642ed2a53" />
    <link rel="stylesheet" type="text/css" href="../../_static/nbsphinx-code-cells.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/furo-extensions.css?digest=30d1aed668e5c3a91c3e3bf6a60b675221979f0e" />
    
    


<style>
  body {
    --color-code-background: #f8f8f8;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-half" viewBox="0 0 24 24">
    <title>Auto light/dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-shadow">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
      <circle cx="12" cy="12" r="9" />
      <path d="M13 12h5" />
      <path d="M13 15h4" />
      <path d="M13 18h1" />
      <path d="M13 9h4" />
      <path d="M13 6h1" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../../index.html"><div class="brand">Introduction to Computer-based Physical Modeling 23 documentation</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../../index.html">
  
  <div class="sidebar-logo-container">
    <img class="sidebar-logo" src="../../_static/mona_logo.png" alt="Logo"/>
  </div>
  
  <span class="sidebar-brand-text">Introduction to Computer-based Physical Modeling 23 documentation</span>
  
</a><form class="sidebar-search-container" method="get" action="../../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <p class="caption" role="heading"><span class="caption-text">Course Information:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../course-info/website.html">This Website</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../course-info/schedule.html">Course Schedule</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../course-info/assignments.html">Assignments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../course-info/exam.html">Exam</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../course-info/resources.html">Resources</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../course-info/instructor.html">Instructor</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Jupyter Notebooks:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../lectures/Intro/overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Intro/1_Introduction2Jupyter.html">Introduction to Jupyter</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Intro/2_NotebookEditor.html">Notebook editor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Intro/3_EditCells.html">Entering code</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Intro/3_EditCells.html#Entering-Markdown">Entering Markdown</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Lecture 1:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../lectures/L1/overview_1.html">Lecture Contents</a></li>
<li class="toctree-l1"><a class="reference internal" href="../L1/1_variables.html">Variables and types</a></li>
<li class="toctree-l1"><a class="reference internal" href="../L1/2_operators.html">Operators and comparisons</a></li>
<li class="toctree-l1"><a class="reference internal" href="../L1/3_datatypes.html">Data Types in Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="../L1/4_modules.html">Modules and namespaces</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../lectures/L1/assignment_1.html">Exercise 1</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Lecture 2:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../lectures/L2/overview_2.html">Lecture Contents</a></li>
<li class="toctree-l1"><a class="reference internal" href="../L2/1_numpy.html">NumPy arrays</a></li>
<li class="toctree-l1"><a class="reference internal" href="../L2/2_plotting.html">Plotting data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../lectures/L2/assignment_2.html">Exercise 2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../L2/25_publication_ready_figures.html">Add On: Making publication ready figures</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Lecture 3:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../lectures/L3/overview_3.html">Lecture Contents</a></li>
<li class="toctree-l1"><a class="reference internal" href="../L2/3_randomnumbers.html">Random numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../L3/1_input_output.html">Input and output</a></li>
<li class="toctree-l1"><a class="reference internal" href="../L3/2_flowcontrol.html">Flow Control</a></li>
<li class="toctree-l1"><a class="reference internal" href="../L3/4_exceptions.html">Exceptions</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../lectures/L3/assignment_3.html">Exercise 3</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul class="simple">
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Lecture 4:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../lectures/L4/overview_4.html">Lecture Contents</a></li>
<li class="toctree-l1"><a class="reference internal" href="../L3/3_functions.html">Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../L4/1_classes.html">Classes and Objects</a></li>
<li class="toctree-l1"><a class="reference internal" href="../L4/2_brownian_motion.html">Brownian Motion</a></li>
<li class="toctree-l1"><a class="reference internal" href="../L4/3_animations.html">Animations</a></li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          
<div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main">
          <div class="admonition note">
<p>This page was generated from <cite>notebooks/L12/2_reinforcement_learning.ipynb</cite>.
<span class="raw-html"><br/><a href="https://colab.research.google.com/github/fcichos/CompSoft23/blob/master/source/notebooks/L12/2_reinforcement_learning.ipynb"><img alt="Binder badge" src="https://img.shields.io/badge/launch-%20colab-green.svg" style="vertical-align:text-bottom"></a></span>
<span class="raw-html"><br/><a href="https://mybinder.org/v2/gh/fcichos/CompSoft23.git/master?labpath=source/notebooks/L12/2_reinforcement_learning.ipynb"><img alt="Binder badge" src="https://img.shields.io/badge/launch-%20myBinder-red.svg" style="vertical-align:text-bottom"></a></span></p>
</div>
<section id="Machine-Learning-and-Neural-Networks">
<h1>Machine Learning and Neural Networks<a class="headerlink" href="#Machine-Learning-and-Neural-Networks" title="Permalink to this heading">#</a></h1>
<p>We are close to the end of the course and covered different applications of Python to physical problems. The course is not intended to teach the physics, but exercise the application of Python. One field, which is increasingly important also in physics is the field of machine learning. Machine learning is the summarizing term for a number of computational procedures to extract useful information from data. We would like to spend the rest of the course to introduce you into a tiny part of machine
learning. We will do that in a way that you calculate as much as possible in pure Python without any additional packages.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[767]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">scipy.constants</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">scipy.sparse</span> <span class="kn">import</span> <span class="n">diags</span>
<span class="kn">from</span> <span class="nn">scipy.fftpack</span> <span class="kn">import</span> <span class="n">fft</span><span class="p">,</span><span class="n">ifft</span>
<span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">sparse</span> <span class="k">as</span> <span class="n">sparse</span>
<span class="kn">from</span> <span class="nn">scipy.sparse</span> <span class="kn">import</span> <span class="n">linalg</span> <span class="k">as</span> <span class="n">ln</span>
<span class="kn">from</span> <span class="nn">time</span> <span class="kn">import</span> <span class="n">sleep</span><span class="p">,</span><span class="n">time</span>
<span class="kn">import</span> <span class="nn">matplotlib.patches</span> <span class="k">as</span> <span class="nn">patches</span>
<span class="kn">from</span> <span class="nn">ipycanvas</span> <span class="kn">import</span> <span class="n">MultiCanvas</span><span class="p">,</span> <span class="n">hold_canvas</span><span class="p">,</span><span class="n">Canvas</span>


<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="o">%</span><span class="k">config</span> InlineBackend.figure_format = &#39;retina&#39;

<span class="c1"># default values for plotting</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s1">&#39;font.size&#39;</span><span class="p">:</span> <span class="mi">16</span><span class="p">,</span>
                     <span class="s1">&#39;axes.titlesize&#39;</span><span class="p">:</span> <span class="mi">18</span><span class="p">,</span>
                     <span class="s1">&#39;axes.labelsize&#39;</span><span class="p">:</span> <span class="mi">16</span><span class="p">,</span>
                     <span class="s1">&#39;axes.labelpad&#39;</span><span class="p">:</span> <span class="mi">14</span><span class="p">,</span>
                     <span class="s1">&#39;lines.linewidth&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
                     <span class="s1">&#39;lines.markersize&#39;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
                     <span class="s1">&#39;xtick.labelsize&#39;</span> <span class="p">:</span> <span class="mi">16</span><span class="p">,</span>
                     <span class="s1">&#39;ytick.labelsize&#39;</span> <span class="p">:</span> <span class="mi">16</span><span class="p">,</span>
                     <span class="s1">&#39;xtick.top&#39;</span> <span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
                     <span class="s1">&#39;xtick.direction&#39;</span> <span class="p">:</span> <span class="s1">&#39;in&#39;</span><span class="p">,</span>
                     <span class="s1">&#39;ytick.right&#39;</span> <span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
                     <span class="s1">&#39;ytick.direction&#39;</span> <span class="p">:</span> <span class="s1">&#39;in&#39;</span><span class="p">,})</span>
</pre></div>
</div>
</div>
<section id="Overview">
<h2>Overview<a class="headerlink" href="#Overview" title="Permalink to this heading">#</a></h2>
<p>Machine learning has its origins long time ago and many of the currently very popular approaches have been developed in the past century. Two things have been stimmulating the current hype of machine learning techniques. One is the computational power that is available already at the level of your smartphone. The second one is the availability of data. Machine learning is divided into different areas, which are denotes as</p>
<ul class="simple">
<li><p>supervised learning: telling the system what is right or wrong</p></li>
<li><p>semi-supervised learning: having only sparse information on what is right or wrong</p></li>
<li><p>unsupervised learning: let the system figure out what is right or wrong</p></li>
</ul>
<p>The graphics below gives a small summary. In our course, we cannot cover all methods. We will focus on <strong>Reinforcement Learning</strong> and <strong>Neural Networks</strong> just to show you, how things could look in Python.</p>
<img alt="../../_images/ml_overview.png" src="../../_images/ml_overview.png" />
<p>Image taken from F. Cichos et al. Nature Machine Intelligence (2020).</p>
</section>
<section id="Reinforcement-Learning">
<h2>Reinforcement Learning<a class="headerlink" href="#Reinforcement-Learning" title="Permalink to this heading">#</a></h2>
<p>Reinforcement learning is learning what to do—how to map situations to actions—so as to maximize a numerical reward signal. The learner or agent is not told which actions to take, as in most forms of machine learning, but instead must discover which actions yield the most reward by trying them. In the most interesting and challenging cases, actions may affect not only the immediate reward but also the next situation and, through that, all subsequent rewards. These two
characteristics—trial-and-error search and delayed reward—are the two most important distinguishing features of reinforcement learning.</p>
<p>It has been around since the 1950s but gained momentum only in 2013 with the demonstrations of DeepMind on how to learn play Atari games like pong. The graphic below shows some of its applications in the field of robotics and gaming.</p>
<img alt="overview_rl" src="../../_images/overview_RL.png" />
<section id="Markov-Decision-Process">
<h3>Markov Decision Process<a class="headerlink" href="#Markov-Decision-Process" title="Permalink to this heading">#</a></h3>
<p>The key element of reinforcement learning is the so-called Markov Decision Process. The Markov decision process (MDP) denotes a formalism of planning actions in the face of uncertainty. A MDP consist formally of</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(S\)</span>: a set of accessible states in the world</p></li>
<li><p><span class="math notranslate nohighlight">\(D\)</span>: an initial distribution to be in a state</p></li>
<li><p><span class="math notranslate nohighlight">\(P_{sa}\)</span>: transition probability between states</p></li>
<li><p><span class="math notranslate nohighlight">\(A\)</span>: A set of possible actions to take in each state</p></li>
<li><p><span class="math notranslate nohighlight">\(\gamma\)</span>: the discount factor, which is a number between 0 and 1</p></li>
<li><p><span class="math notranslate nohighlight">\(R\)</span>: A reward function</p></li>
</ul>
<p>We begin in an initial state <span class="math notranslate nohighlight">\(s_{i,j}\)</span> drawn from the distribution <span class="math notranslate nohighlight">\(D\)</span>. At each time step <span class="math notranslate nohighlight">\(t\)</span>, we then have to pick an action, for example <span class="math notranslate nohighlight">\(a_1(t)\)</span> , as a result of which our state transitions to some state <span class="math notranslate nohighlight">\(s_{i,j+1}\)</span>. The states do not nessecarily correspond to spatial positions, however, as we talk about the gridworld later we may use this example to understand the procedures.</p>
<img alt="gw_with_path" src="../../_images/gw_with_path.png" />
<p>By repeatedly picking actions, we traverse some sequence of states</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[s_{0,0}\rightarrow s_{0,1}\rightarrow s_{1,1}+\ldots\]</div>
</div>
<p>Our total reward is then the sum of discounted rewards along this sequence of states</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[R(s_{0,0})+\gamma R(s_{0,1})+ \gamma^2 R(s_{1,1})+ \ldots\]</div>
</div>
<p>Here, the discount factor <span class="math notranslate nohighlight">\(\gamma\)</span>, which is typically strictly less than one, causes rewards obtained immediately to be more valuable than those obtained in the future.</p>
<p>In reinforcement learning, our goal is to find a way of choosing actions <span class="math notranslate nohighlight">\(a_0\)</span>,<span class="math notranslate nohighlight">\(a_1, \ldots\)</span> over time, so as to maximize the expected value of the rewards. The sequence of actions that realizes the maximum reward is called the optimal policy <span class="math notranslate nohighlight">\(\pi^{*}\)</span>. A sequence of actions in general is called a policy <span class="math notranslate nohighlight">\(\pi\)</span>.</p>
<section id="Methods-or-RL">
<h4>Methods or RL<a class="headerlink" href="#Methods-or-RL" title="Permalink to this heading">#</a></h4>
<p>There are different methods available to find the optimal policy. If we know the transition probabilities <span class="math notranslate nohighlight">\(P_{sa}\)</span> the methods are called model-based algorithms. The so-called value interation procedure would be one of those methods, which we, however, do not consider.</p>
<p>If we don’t know the transition probabilities, then its model-free RL. We will have a look at one of those mode-free algorithms, which is Q‐learning.</p>
<p>In Q-learning, the value of an action in a state is measured by its Q-value. The expectation value <span class="math notranslate nohighlight">\(E\)</span> of the rewards with and initial state and action for a given policy is the Q-function or Q-value.</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[Q^{\pi}(s,a)=E[R(s_{0},a_{0})+\gamma R(s_{1},a_{1})+ \gamma^2 R(s_{2},a_{2})+ \ldots | s_{0}=s,a_{0}=a,a_{t}=\pi(s_{t})]\]</div>
</div>
<p>This sounds complicated but is in principle easy. There is a Q-value for all actions of each state. Thus if we have 4 actions an 25 states, we have to store in total 100 Q-values.</p>
<p>For the optimal sequence of actions - for the best way to go - this Q value becomes a maximum.</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[Q^{*}(s,a)=\max_{\pi}Q^{\pi}(s,a)\]</div>
</div>
<p>The policy which gives the sequence of actions to be carried out to get the maximum reward is then calculated by</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\pi^{*}(s)={\rm argmax_{a}}Q^{*}(s,a)\]</div>
</div>
<p>The <strong>Q-learning</strong> algorithm is now an iterative procedure of updating the Q-value of each state and action which converges to the optimal policy <span class="math notranslate nohighlight">\(\pi^{*}\)</span>. It is given by</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[Q_{t+\Delta t}(s,a)  = Q_t(s,a) + \alpha\big[R(s) + \gamma \max_{a'}Q_t(s',a')-Q_t(s,a)\big]\]</div>
</div>
<p>This states, that the current Q-value of the current state <span class="math notranslate nohighlight">\(s\)</span> and the taken action <span class="math notranslate nohighlight">\(a\)</span> for the next step is calculated from its current value <span class="math notranslate nohighlight">\(Q_t(s,a)\)</span> plus an update value. This update value is calculated by multiplying the so-called learing rate <span class="math notranslate nohighlight">\(\alpha\)</span> with the reward <span class="math notranslate nohighlight">\(R\)</span> obtained when taking the action plus a discounted value (discounted by <span class="math notranslate nohighlight">\(\gamma\)</span>) when taking the best action in the next state <span class="math notranslate nohighlight">\(\gamma \max_{a'}Q_t(s',a')\)</span>. This is the procedure
we would like to explore in a small Python program, which is not too difficult.</p>
</section>
</section>
</section>
<section id="Navigating-a-Grid-World">
<h2>Navigating a Grid World<a class="headerlink" href="#Navigating-a-Grid-World" title="Permalink to this heading">#</a></h2>
<p>For our Python course we will have a look at the standard problem of reinforcement learning, which is the navigation in a grid world. Each of the grid cells below represents a state <span class="math notranslate nohighlight">\(s\)</span> in which an object could reside. In each of these states, the object can take several actions. If it may step to left, right, up or down, there are 4 actions, which we may call <span class="math notranslate nohighlight">\(a_{1},a_{2},a_{3}\)</span> and <span class="math notranslate nohighlight">\(a_{4}\)</span>.</p>
<p>This image below shows our gridworld, with 25 states, where the shaded state is the goal state where we want the agent to go to independent of its intial state.</p>
<img alt="gridworld" src="../../_images/gridworld.png" />
<p>In each of these state, we have 4 possible action as depicted below</p>
<img alt="actions" src="../../_images/state_n_action.png" />
<section id="Initialize-Reinforcement-Learning">
<h3>Initialize Reinforcement Learning<a class="headerlink" href="#Initialize-Reinforcement-Learning" title="Permalink to this heading">#</a></h3>
<p>At first we would like to initialize our problem. We have as depicted above 25 states, where one state is the goal state. We would like to use 4 actions to move between the states so our Q-value matrix has 100 entries. We would like to give a penalty of <span class="math notranslate nohighlight">\(R=-1\)</span> for all states except for the goal state where we give a reward of <span class="math notranslate nohighlight">\(R=10\)</span>.</p>
<p>Our agent shall learn with a learning rate of <span class="math notranslate nohighlight">\(\alpha=0.5\)</span> and we will discount future rewards with <span class="math notranslate nohighlight">\(\gamma=0.5\)</span>.</p>
<p>There is one tiny detail, which is useful to understand. If we run into a certain strategy and this is not the optimal strategy, it is difficult for the algorithm to choose a different action. Therefore the so called <span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy factor is introduced. It tells you at which fraction of events in a state a random action is to be chosen over the action with the larges Q-value. We will set this <span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy value to 0.2, meaning that 20% of the actions are chosen randomly.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[805]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_actions</span><span class="o">=</span><span class="mi">4</span>
<span class="n">n_rows</span><span class="o">=</span><span class="n">n_columns</span><span class="o">=</span><span class="mi">5</span>

<span class="n">Q</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n_rows</span><span class="p">,</span><span class="n">n_columns</span><span class="p">,</span><span class="n">n_actions</span><span class="p">)</span>
<span class="n">R</span><span class="o">=-</span><span class="mi">1</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">n_rows</span><span class="p">,</span><span class="n">n_columns</span><span class="p">])</span>
<span class="n">R</span><span class="p">[</span><span class="n">n_rows</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">n_columns</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">=</span><span class="mi">10</span>

<span class="n">e_greedy</span><span class="o">=</span><span class="mf">0.2</span>
<span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span>
<span class="n">gamma</span><span class="o">=</span><span class="mf">0.5</span>
</pre></div>
</div>
</div>
</section>
<section id="List-of-actions">
<h3>List of actions<a class="headerlink" href="#List-of-actions" title="Permalink to this heading">#</a></h3>
<p>The actions, which we can take in each state are defined by 2-d vectors here which increase either the row or the column index in our gridworld.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[806]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">acl</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">]])</span>
</pre></div>
</div>
</div>
</section>
<section id="Initial-state">
<h3>Initial state<a class="headerlink" href="#Initial-state" title="Permalink to this heading">#</a></h3>
<p>We chose the initial state from which we start randomly. We also initialize a list, where we register the sum of all Q-values. This is helpful to monitor the convergence of our algorithm.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[807]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">curr_state</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">([</span><span class="n">n_rows</span><span class="p">,</span><span class="n">n_columns</span><span class="p">])</span>
<span class="n">curr_state</span>
<span class="n">ep</span><span class="o">=</span><span class="mi">0</span>
<span class="n">qsum</span><span class="o">=</span><span class="p">[]</span>
</pre></div>
</div>
</div>
</section>
<section id="Reinforcement-Learning-Loop">
<h3>Reinforcement Learning Loop<a class="headerlink" href="#Reinforcement-Learning-Loop" title="Permalink to this heading">#</a></h3>
<p>The cell below is all you need for the learning how to navigate the grid world.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[808]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">):</span>

    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">()</span><span class="o">&gt;</span><span class="n">e_greedy</span><span class="p">:</span>
        <span class="n">action</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">Q</span><span class="p">[</span><span class="n">curr_state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">curr_state</span><span class="p">[</span><span class="mi">1</span><span class="p">],:])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">action</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>

    <span class="n">next_state</span><span class="o">=</span><span class="n">curr_state</span><span class="o">+</span><span class="n">acl</span><span class="p">[</span><span class="n">action</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">next_state</span><span class="o">&lt;=</span><span class="n">n_rows</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="ow">and</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">next_state</span><span class="o">&gt;=</span><span class="mi">0</span><span class="p">):</span> <span class="c1">## normal states</span>
        <span class="n">next_action</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">Q</span><span class="p">[</span><span class="n">next_state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">next_state</span><span class="p">[</span><span class="mi">1</span><span class="p">],:])</span>
        <span class="n">next_Q</span><span class="o">=</span><span class="n">Q</span><span class="p">[</span><span class="n">next_state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">next_state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">next_action</span><span class="p">]</span>
        <span class="n">Q</span><span class="p">[</span><span class="n">curr_state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">curr_state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">action</span><span class="p">]</span><span class="o">=</span><span class="n">Q</span><span class="p">[</span><span class="n">curr_state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">curr_state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">action</span><span class="p">]</span><span class="o">+</span><span class="n">alpha</span><span class="o">*</span><span class="p">(</span><span class="n">R</span><span class="p">[</span><span class="n">curr_state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">curr_state</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span><span class="o">+</span><span class="n">gamma</span><span class="o">*</span><span class="n">next_Q</span><span class="o">-</span><span class="n">Q</span><span class="p">[</span><span class="n">curr_state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">curr_state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">action</span><span class="p">])</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">next_state</span><span class="o">==</span><span class="n">n_rows</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span> <span class="c1">## the goal state, episode ends</span>
            <span class="n">next_state</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">([</span><span class="n">n_rows</span><span class="p">,</span><span class="n">n_columns</span><span class="p">])</span>
            <span class="n">ep</span><span class="o">+=</span><span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">Q</span><span class="p">[</span><span class="n">curr_state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">curr_state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">action</span><span class="p">]</span><span class="o">=</span><span class="n">Q</span><span class="p">[</span><span class="n">curr_state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">curr_state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">action</span><span class="p">]</span><span class="o">+</span><span class="n">alpha</span><span class="o">*</span><span class="p">(</span><span class="o">-</span><span class="mi">100</span><span class="o">-</span><span class="n">Q</span><span class="p">[</span><span class="n">curr_state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">curr_state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">action</span><span class="p">]);</span>
        <span class="n">next_state</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">([</span><span class="n">n_rows</span><span class="p">,</span><span class="n">n_columns</span><span class="p">])</span>
        <span class="n">ep</span><span class="o">+=</span><span class="mi">1</span>
    <span class="n">qsum</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">Q</span><span class="p">))</span>

    <span class="c1">#curr_action=next_action</span>
    <span class="n">curr_state</span><span class="o">=</span><span class="n">next_state</span>
<br/></pre></div>
</div>
</div>
</section>
<section id="Convergence-of-the-Q-learning">
<h3>Convergence of the Q-learning<a class="headerlink" href="#Convergence-of-the-Q-learning" title="Permalink to this heading">#</a></h3>
<p>The convergence of our learning is best judged from the sum of all Q-values in the matrix. This should converge to a negative value as most of the time our agent is getting the penalty <span class="math notranslate nohighlight">\(R=-1\)</span> and only sparsely <span class="math notranslate nohighlight">\(R=10\)</span> at the goal.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[809]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">qsum</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;transition&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\sum Q$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/notebooks_L12_2_reinforcement_learning_49_0.png" class="no-scaled-link" src="../../_images/notebooks_L12_2_reinforcement_learning_49_0.png" style="width: 454px; height: 280px;" />
</div>
</div>
</section>
<section id="Policy">
<h3>Policy<a class="headerlink" href="#Policy" title="Permalink to this heading">#</a></h3>
<p>The policy is obtained by taking the best actions with the larges Q-value from our Q-matrix.</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\pi^{*}(s)={\rm argmax_{a}}Q^{*}(s,a)\]</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[810]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">policy</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">Q</span><span class="p">[:,:,:],</span><span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">policy</span><span class="p">[</span><span class="n">n_rows</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">n_columns</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">=-</span><span class="mi">1</span>
</pre></div>
</div>
</div>
</section>
<section id="Plot-the-policy">
<h3>Plot the policy<a class="headerlink" href="#Plot-the-policy" title="Permalink to this heading">#</a></h3>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[811]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">f</span><span class="p">,</span><span class="n">ax</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>

<span class="n">f</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
<span class="n">i</span><span class="o">=</span><span class="mi">0</span>
<span class="k">for</span> <span class="n">k</span><span class="p">,</span><span class="n">y</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="n">n_rows</span><span class="o">*</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">)):</span>
    <span class="k">for</span> <span class="n">l</span><span class="p">,</span><span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="n">n_columns</span><span class="o">*</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">)):</span>
        <span class="k">if</span> <span class="n">policy</span><span class="p">[</span><span class="n">k</span><span class="p">,</span><span class="n">l</span><span class="p">]</span><span class="o">!=-</span><span class="mi">1</span><span class="p">:</span>
            <span class="n">vec</span><span class="o">=</span><span class="n">acl</span><span class="p">[</span><span class="n">policy</span><span class="p">[</span><span class="n">k</span><span class="p">,</span><span class="n">l</span><span class="p">]]</span><span class="o">*</span><span class="mi">2</span>
            <span class="k">if</span> <span class="n">i</span><span class="o">!=</span><span class="n">n_rows</span><span class="o">*</span><span class="n">n_columns</span><span class="p">:</span>
                <span class="n">plt</span><span class="o">.</span><span class="n">arrow</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="n">vec</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="n">y</span><span class="o">-</span><span class="n">vec</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="n">vec</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">vec</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">fc</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">,</span><span class="n">head_width</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">head_length</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mf">0.01</span> <span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="n">rect</span><span class="o">=</span><span class="n">patches</span><span class="o">.</span><span class="n">Rectangle</span><span class="p">((</span><span class="n">x</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="n">y</span><span class="o">-</span><span class="mi">3</span><span class="p">),</span> <span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">rect</span><span class="p">)</span>
        <span class="n">i</span><span class="o">=</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_rows</span><span class="o">*</span><span class="n">n_columns</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_rows</span><span class="o">*</span><span class="n">n_columns</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">6</span><span class="o">*</span><span class="n">n_columns</span><span class="p">)</span><span class="n">b</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">6</span><span class="o">*</span><span class="n">n_rows</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span><span class="n">ls</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>

<span class="n">f</span><span class="o">.</span><span class="n">axes</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_ticklabels</span><span class="p">([])</span>
<span class="n">f</span><span class="o">.</span><span class="n">axes</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_ticklabels</span><span class="p">([])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/notebooks_L12_2_reinforcement_learning_54_0.png" class="no-scaled-link" src="../../_images/notebooks_L12_2_reinforcement_learning_54_0.png" style="width: 349px; height: 340px;" />
</div>
</div>
</section>
</section>
<section id="Where-to-go-from-here">
<h2>Where to go from here<a class="headerlink" href="#Where-to-go-from-here" title="Permalink to this heading">#</a></h2>
<p>If you want to know more about Reinforcement Learning, have a look at the <a class="reference external" href="http://incompleteideas.net/book/bookdraft2017nov5.pdf">book</a> of Sutton and Barto.</p>
<script type="application/vnd.jupyter.widget-state+json">
{"state": {}, "version_major": 2, "version_minor": 0}
</script></section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          
          
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2023, Frank Cichos
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            <div class="last-updated">
              Last updated on May 02, 2023</div>
          </div>
          <div class="right-details">
            
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">Machine Learning and Neural Networks</a><ul>
<li><a class="reference internal" href="#Overview">Overview</a></li>
<li><a class="reference internal" href="#Reinforcement-Learning">Reinforcement Learning</a><ul>
<li><a class="reference internal" href="#Markov-Decision-Process">Markov Decision Process</a><ul>
<li><a class="reference internal" href="#Methods-or-RL">Methods or RL</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#Navigating-a-Grid-World">Navigating a Grid World</a><ul>
<li><a class="reference internal" href="#Initialize-Reinforcement-Learning">Initialize Reinforcement Learning</a></li>
<li><a class="reference internal" href="#List-of-actions">List of actions</a></li>
<li><a class="reference internal" href="#Initial-state">Initial state</a></li>
<li><a class="reference internal" href="#Reinforcement-Learning-Loop">Reinforcement Learning Loop</a></li>
<li><a class="reference internal" href="#Convergence-of-the-Q-learning">Convergence of the Q-learning</a></li>
<li><a class="reference internal" href="#Policy">Policy</a></li>
<li><a class="reference internal" href="#Plot-the-policy">Plot the policy</a></li>
</ul>
</li>
<li><a class="reference internal" href="#Where-to-go-from-here">Where to go from here</a></li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/sphinx_highlight.js"></script>
    <script src="../../_static/scripts/furo.js"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.1/dist/embed-amd.js"></script>
    <script>window.MathJax = {"TeX": {"equationNumbers": {"autoNumber": "AMS", "useLabelIds": true}}, "tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    </body>
</html>